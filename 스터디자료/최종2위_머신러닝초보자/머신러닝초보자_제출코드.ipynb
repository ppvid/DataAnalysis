{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c1d6ec-f420-4b0f-b982-19a006e24935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필요한 모듈 설치 \n",
    "!pip install --quiet xgboost\n",
    "!pip install --quiet catboost\n",
    "!pip install --quiet lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3664b9-73cd-4566-b927-7adc8011a81d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (382341307.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    from stcipy.stats impor pearsonr\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from stcipy.stats impor pearsonr\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93332e2e-f277-49d6-ad2f-8bd3078c730f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Load\n",
    "train_df = pd.read_csv('Untitled Folder/train.csv')\n",
    "test_df = pd.read_csv(\"Untitled Folder/test.csv\")\n",
    "submit = pd.read_csv('Untitled Folder/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3da166-21ea-4c36-8f72-bfa8c86403a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# PRODUCT_ID 와 TIMESTAMP 삭제\n",
    "train_df = train_df.drop(columns=['PRODUCT_ID'])\n",
    "test_df = test_df.drop(columns=['PRODUCT_ID'])\n",
    "\n",
    "# 베이스 라인을 참조하여 라벨 인코딩 \n",
    "qual_col = ['LINE', 'PRODUCT_CODE']\n",
    "\n",
    "for i in qual_col:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(train_df[i])\n",
    "    train_df[i] = le.transform(train_df[i])\n",
    "    \n",
    "    for label in np.unique(test_df[i]): \n",
    "        if label not in le.classes_: \n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    test_df[i] = le.transform(test_df[i]) \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f47ec2-7a84-4545-b1a2-629703c4d529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with all missing values:\n",
      "['X_2628', 'X_2629', 'X_2630', 'X_2631', 'X_2632', 'X_2633', 'X_2634', 'X_2635', 'X_2636', 'X_2637', 'X_2638', 'X_2639', 'X_2640', 'X_2641', 'X_2642', 'X_2643', 'X_2644', 'X_2645', 'X_2646', 'X_2647', 'X_2648', 'X_2649', 'X_2650', 'X_2651', 'X_2652', 'X_2653', 'X_2654', 'X_2655', 'X_2656', 'X_2657', 'X_2658', 'X_2659', 'X_2660', 'X_2661', 'X_2662', 'X_2663', 'X_2664', 'X_2665', 'X_2666', 'X_2667', 'X_2668', 'X_2669', 'X_2670', 'X_2671', 'X_2672', 'X_2673', 'X_2674', 'X_2675', 'X_2676', 'X_2677', 'X_2678', 'X_2679', 'X_2680', 'X_2681', 'X_2682', 'X_2683', 'X_2684', 'X_2685', 'X_2686', 'X_2687', 'X_2688', 'X_2689', 'X_2690', 'X_2691', 'X_2692', 'X_2693', 'X_2694', 'X_2695', 'X_2696', 'X_2697', 'X_2698', 'X_2699', 'X_2838', 'X_2844']\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "# nan값만 가지는 column\n",
    "all_nan_cols = train_df.columns[train_df.isna().all()].tolist()\n",
    "\n",
    "print('Columns with all missing values:')\n",
    "print(all_nan_cols)\n",
    "print(len(all_nan_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892e03ce-7576-4645-b97c-0637a70f9258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with only one unique value:\n",
      "['X_3', 'X_6', 'X_10', 'X_14', 'X_19', 'X_29', 'X_31', 'X_33', 'X_34', 'X_35', 'X_36', 'X_37', 'X_67', 'X_68', 'X_69', 'X_70', 'X_71', 'X_72', 'X_74', 'X_75', 'X_76', 'X_77', 'X_78', 'X_79', 'X_80', 'X_81', 'X_82', 'X_83', 'X_84', 'X_85', 'X_89', 'X_91', 'X_92', 'X_96', 'X_100', 'X_104', 'X_108', 'X_112', 'X_116', 'X_122', 'X_135', 'X_142', 'X_145', 'X_147', 'X_154', 'X_158', 'X_162', 'X_166', 'X_170', 'X_174', 'X_178', 'X_182', 'X_186', 'X_187', 'X_188', 'X_190', 'X_191', 'X_192', 'X_193', 'X_194', 'X_195', 'X_232', 'X_234', 'X_235', 'X_239', 'X_241', 'X_244', 'X_249', 'X_252', 'X_255', 'X_259', 'X_260', 'X_264', 'X_274', 'X_278', 'X_282', 'X_312', 'X_313', 'X_314', 'X_315', 'X_316', 'X_317', 'X_319', 'X_322', 'X_323', 'X_324', 'X_325', 'X_326', 'X_327', 'X_328', 'X_329', 'X_330', 'X_334', 'X_336', 'X_340', 'X_342', 'X_343', 'X_347', 'X_351', 'X_355', 'X_359', 'X_363', 'X_369', 'X_375', 'X_376', 'X_377', 'X_378', 'X_381', 'X_382', 'X_383', 'X_390', 'X_391', 'X_392', 'X_409', 'X_410', 'X_411', 'X_424', 'X_425', 'X_426', 'X_427', 'X_429', 'X_430', 'X_431', 'X_432', 'X_433', 'X_434', 'X_466', 'X_467', 'X_480', 'X_487', 'X_503', 'X_505', 'X_508', 'X_512', 'X_519', 'X_551', 'X_558', 'X_561', 'X_567', 'X_583', 'X_617', 'X_620', 'X_631', 'X_633', 'X_634', 'X_636', 'X_638', 'X_639', 'X_640', 'X_641', 'X_642', 'X_650', 'X_672', 'X_673', 'X_676', 'X_691', 'X_692', 'X_695', 'X_715', 'X_729', 'X_732', 'X_743', 'X_749', 'X_759', 'X_760', 'X_761', 'X_764', 'X_776', 'X_777', 'X_778', 'X_836', 'X_843', 'X_844', 'X_849', 'X_859', 'X_872', 'X_873', 'X_874', 'X_879', 'X_880', 'X_881', 'X_886', 'X_887', 'X_888', 'X_889', 'X_910', 'X_912', 'X_950', 'X_951', 'X_952', 'X_969', 'X_970', 'X_971', 'X_987', 'X_988', 'X_989', 'X_992', 'X_1003', 'X_1004', 'X_1005', 'X_1006', 'X_1007', 'X_1008', 'X_1020', 'X_1021', 'X_1022', 'X_1023', 'X_1024', 'X_1025', 'X_1070', 'X_1092', 'X_1103', 'X_1119', 'X_1130', 'X_1137', 'X_1146', 'X_1157', 'X_1206', 'X_1216', 'X_1219', 'X_1248', 'X_1249', 'X_1250', 'X_1251', 'X_1252', 'X_1253', 'X_1255', 'X_1293', 'X_1298', 'X_1309', 'X_1311', 'X_1312', 'X_1314', 'X_1316', 'X_1317', 'X_1318', 'X_1319', 'X_1320', 'X_1328', 'X_1361', 'X_1362', 'X_1363', 'X_1364', 'X_1367', 'X_1392', 'X_1393', 'X_1394', 'X_1395', 'X_1396', 'X_1399', 'X_1426', 'X_1457', 'X_1487', 'X_1502', 'X_1503', 'X_1504', 'X_1522', 'X_1531', 'X_1537', 'X_1571', 'X_1572', 'X_1573', 'X_1574', 'X_1575', 'X_1576', 'X_1577', 'X_1578', 'X_1579', 'X_1580', 'X_1581', 'X_1582', 'X_1644', 'X_1645', 'X_1675', 'X_1676', 'X_1677', 'X_1680', 'X_1681', 'X_1683', 'X_1684', 'X_1685', 'X_1687', 'X_1688', 'X_1689', 'X_1691', 'X_1692', 'X_1693', 'X_1695', 'X_1698', 'X_1699', 'X_1700', 'X_1701', 'X_1702', 'X_1707', 'X_1711', 'X_1721', 'X_1722', 'X_1723', 'X_1724', 'X_1729', 'X_1731', 'X_1733', 'X_1749', 'X_1753', 'X_1754', 'X_1759', 'X_1760', 'X_1765', 'X_1766', 'X_1771', 'X_1772', 'X_1777', 'X_1778', 'X_1783', 'X_1784', 'X_1789', 'X_1790', 'X_1795', 'X_1796', 'X_1801', 'X_1802', 'X_1803', 'X_1807', 'X_1808', 'X_1823', 'X_1827', 'X_1828', 'X_1835', 'X_1836', 'X_1837', 'X_1838', 'X_1839', 'X_1840', 'X_1841', 'X_1842', 'X_1844', 'X_1845', 'X_1846', 'X_1847', 'X_1848', 'X_1851', 'X_1852', 'X_1869', 'X_1870', 'X_1871', 'X_1872', 'X_2052', 'X_2053', 'X_2054', 'X_2055', 'X_2188', 'X_2189', 'X_2419', 'X_2420', 'X_2425', 'X_2462', 'X_2549', 'X_2550', 'X_2732', 'X_2733', 'X_2738', 'X_2775', 'X_2898', 'X_2900', 'X_2902', 'X_2909', 'X_2910', 'X_2911', 'X_2912', 'X_2915', 'X_2916', 'X_2917', 'X_2972', 'X_2973', 'X_2974', 'X_2987', 'X_2989', 'X_2998', 'X_2999', 'X_3000', 'X_3001', 'X_3004', 'X_3005', 'X_3006', 'X_3061', 'X_3062', 'X_3063', 'X_3071', 'X_3134', 'X_3136', 'X_3140', 'X_3141', 'X_3142', 'X_3143', 'X_3146', 'X_3147', 'X_3148', 'X_3149', 'X_3150', 'X_3151', 'X_3204', 'X_3205', 'X_3207', 'X_3218', 'X_3220', 'X_3223', 'X_3224', 'X_3225', 'X_3229', 'X_3230', 'X_3231', 'X_3232', 'X_3233', 'X_3234', 'X_3235', 'X_3236', 'X_3237', 'X_3238', 'X_3239', 'X_3240', 'X_3241', 'X_3242', 'X_3243', 'X_3244', 'X_3245', 'X_3246', 'X_3247', 'X_3248', 'X_3249', 'X_3253', 'X_3258', 'X_3261', 'X_3268', 'X_3269', 'X_3270', 'X_3274', 'X_3275', 'X_3276', 'X_3277', 'X_3278', 'X_3279', 'X_3280', 'X_3281', 'X_3282', 'X_3283', 'X_3284', 'X_3285', 'X_3286', 'X_3287', 'X_3288', 'X_3289', 'X_3290', 'X_3291', 'X_3292', 'X_3293', 'X_3294', 'X_3298', 'X_3301', 'X_3304', 'X_3305', 'X_3306', 'X_3307']\n",
      "462\n"
     ]
    }
   ],
   "source": [
    "# unique 값이 하나만 존재하는 column\n",
    "const_cols = train_df.columns[train_df.nunique() == 1].tolist()\n",
    "\n",
    "print('Columns with only one unique value:')\n",
    "print(const_cols)\n",
    "print(len(const_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910d8b3d-aeae-4aff-9af7-d0705a55fdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2794\n"
     ]
    }
   ],
   "source": [
    "# train data에서 nan값만 가지는 column, unique 값이 하나만 존재하는 column 삭제(훈련에 방해 예상)\n",
    "train_df = train_df.drop(all_nan_cols, axis=1)\n",
    "train_df = train_df.drop(const_cols, axis=1)\n",
    "\n",
    "print(len(train_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1ceb0b-5020-4114-96ec-ef649381500e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test data에서 nan값만 가지는 column, unique 값이 하나만 존재하는 column 삭제(train과 동일 처리)\n",
    "test_df = test_df.drop(all_nan_cols, axis=1)\n",
    "test_df = test_df.drop(const_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1eb2b2b-2e7c-4bb3-aae9-39461cd8075b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 전처리 \n",
    "# Y_Class 별로 나눔 \n",
    "train_class = []\n",
    "for i in range(3) :\n",
    "  train_class.append(train_df.loc[train_df[\"Y_Class\"].isin([i])])\n",
    "\n",
    "# Y_Class 별로 나눈 데이터에서 다시 LINE별로 나눔 \n",
    "line = train_df[\"LINE\"].unique()\n",
    "train_class_line = []\n",
    "for df in train_class :\n",
    "  for l in line :\n",
    "    train_class_line.append(df.loc[df[\"LINE\"].isin([l])])\n",
    "\n",
    "# Y_Class, LINE 별로 나눈 데이터에서 다시 PRODUCT_CODE로 나눔 \n",
    "code = train_df[\"PRODUCT_CODE\"].unique()\n",
    "train_class_line_code = [] \n",
    "for df in train_class_line :\n",
    "  for c in code :\n",
    "    if (len(df.loc[df[\"PRODUCT_CODE\"].isin([c])]) != 0) : \n",
    "      train_class_line_code.append(df.loc[df[\"PRODUCT_CODE\"].isin([c])])\n",
    "\n",
    "# 0으로 결측치 처리하기로 결정 \n",
    "for i in range(len(train_class_line_code)) :\n",
    "  train_class_line_code[i] =  train_class_line_code[i].fillna(0)\n",
    "\n",
    "# 나눴던 데이터를 다시 합침\n",
    "train = pd.concat(train_class_line_code)\n",
    "train = train.reset_index(drop = True)\n",
    "\n",
    "# Y_class와 Y_Quality에 target을 저장 후 드람\n",
    "y_class = train[\"Y_Class\"]\n",
    "y_quality = train[\"Y_Quality\"]\n",
    "train = train.drop(columns=['Y_Class', 'Y_Quality'])\n",
    "\n",
    "X = train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03819db-edb0-46c8-a104-8655cf9f0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train에서 결측치를 0으로 대체했기에 마찬가지로 test에서도 결측치 0으로 대체\n",
    "test_df = test_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "451ed72f-7eaa-48ad-87a0-99ebf60eb4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 이상치 처리 \n",
    "def clean_column(col, reference_col=None):\n",
    "    if reference_col is None:\n",
    "        reference_col = col\n",
    "        \n",
    "    z_scores = (reference_col - reference_col.mean()) / reference_col.std()\n",
    "    col_cleaned = col.mask(abs(z_scores) > 3)\n",
    "\n",
    "    most_frequent_value = reference_col.mode()[0]\n",
    "\n",
    "    col_filled = col_cleaned.fillna(most_frequent_value)\n",
    "\n",
    "    return col_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41fbf42-36d0-4ec3-b7c8-c684046438d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 이상치 처리 train, test에 동일하게 적용\n",
    "X_t = X.apply(clean_column)\n",
    "test_df = test_df.apply(lambda col: clean_column(col, reference_col=X[col.name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffdbaf64-31b6-4fb0-8dcf-bfc5fc4ebe6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rbs = RobustScaler()\n",
    "train_scaled = rbs.fit_transform(X_t)\n",
    "test_scaled = rbs.transform(test_df)\n",
    "\n",
    "X_train = pd.DataFrame(train_scaled)\n",
    "X_test = pd.DataFrame(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a3f4984-c73e-427d-891a-89f906333889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 스태킹 앙상블을 진행하고 최종적으로 예측까지 진행하는 함수 \n",
    "def stacking_and_pred(final_X) :\n",
    "  # 최종 학습시킬 train데이터 fianl_X 나눔 \n",
    "  X_train, X_val, y_train, y_val = train_test_split(final_X, y_class, test_size=0.2, random_state=37)\n",
    "\n",
    "  # 가장 보편적으로 많이 사용하는 5가지 모델을 베이스 모델로 설정 \n",
    "  base_models = [\n",
    "      GradientBoostingClassifier(random_state=37),\n",
    "      RandomForestClassifier(n_estimators=2000, random_state=37),\n",
    "      xgb.XGBClassifier(random_state = 37),\n",
    "      LGBMClassifier(random_state = 37),\n",
    "      CatBoostClassifier(random_state=37,verbose=0)\n",
    "  ]\n",
    "  \n",
    "  # 메타 모델로 xgboost 선정 \n",
    "  meta_model = xgb.XGBClassifier(random_state = 37)\n",
    "\n",
    "  # K_Fold를 통한 스태킹 앙상블 진행\n",
    "  n_splits = 5\n",
    "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=37)\n",
    "\n",
    "  X_train_meta = np.zeros((X_train.shape[0], len(base_models)))\n",
    "\n",
    "  # validation으로 메타모델의 f1_score 검증\n",
    "  for i, model in enumerate(base_models):\n",
    "      for train_idx, val_idx in kf.split(X_train):\n",
    "          X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "          y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "          model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "          y_val_pred = model.predict(X_val_fold)\n",
    "\n",
    "          X_train_meta[val_idx, i] = y_val_pred.ravel()\n",
    "\n",
    "  meta_model.fit(X_train_meta, y_train)\n",
    "\n",
    "  X_val_meta = np.zeros((X_val.shape[0], len(base_models)))\n",
    "  for i, model in enumerate(base_models):\n",
    "      model.fit(X_train, y_train)\n",
    "\n",
    "      y_val_pred = model.predict(X_val)\n",
    "      X_val_meta[:, i] = y_val_pred.ravel()\n",
    "\n",
    "  y_pred = meta_model.predict(X_val_meta)\n",
    "\n",
    "  f1Score = f1_score(y_val, y_pred, average='macro')\n",
    "  print(\"F1 score:\", f1Score)\n",
    "\n",
    "  # 각 베이스 모델의 test데이터 예측 결과를 담을 변수 \n",
    "  X_test_meta = np.zeros((new_test.shape[0], len(base_models)))\n",
    "\n",
    "  # 전체 데이터로 학습을 시킨 후 예측한 후 X_test_meta에 값 저장 \n",
    "  for i, model in enumerate(base_models):\n",
    "      model.fit(final_X, y_class)\n",
    "      y_test_pred = model.predict(new_test)\n",
    "      X_test_meta[:, i] = y_test_pred.ravel()\n",
    "\n",
    "  # 메타 모델로 최종 y_pred 예측 \n",
    "  y_pred = meta_model.predict(X_test_meta)\n",
    "  # 이후 결과 및 f1_socre 반환 \n",
    "  return y_pred, f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3adb384-a811-4471-8b09-85b5a3ad41ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 가장 좋은 성능을 보여줄 feature 개수를 찾기 위한 함수 \n",
    "def find(X, bound) : \n",
    "  # Y_Class와 Y_Quality와 나머지 feature들의 pearsons 상관계수를 활용 해 feature_selection 진행 \n",
    "  correlations = []\n",
    "  for feature in X.columns:\n",
    "    correlation, _ = pearsonr(X[feature], y_class)\n",
    "    correlations.append(correlation)\n",
    "\n",
    "  correlations2 = []\n",
    "  for feature in X.columns:\n",
    "      correlation, _ = pearsonr(X[feature], y_quality)\n",
    "      correlations2.append(correlation)\n",
    "\n",
    "  # 3로 나눴을 때 성능이 준수하여 3로 결정 \n",
    "  k = len(X.columns) // 3\n",
    "  top_k_indices = sorted(range(len(correlations)), key=lambda i: abs(correlations[i]), reverse=True)[:k]\n",
    "  top_k_features = X.columns[top_k_indices]\n",
    "\n",
    "  top_k_indices = sorted(range(len(correlations2)), key=lambda i: abs(correlations2[i]), reverse=True)[:k]\n",
    "  top_k_features2 = X.columns[top_k_indices]\n",
    "\n",
    "  top_features = list(OrderedDict.fromkeys(list(top_k_features) + list(top_k_features2)))\n",
    "  new_X = X[top_features]\n",
    "\n",
    "  # train으로 여러 모델을 돌려 본 결과 validation이 RandomForest가 가장 높았음 \n",
    "  model = RandomForestClassifier(random_state=37)\n",
    "  model.fit(new_X, y_class)\n",
    "\n",
    "  # 학습 후, feature_importance로 한번 더 feature selection 진행 \n",
    "  importances = model.feature_importances_\n",
    "  feature_importances = pd.DataFrame({'feature': new_X.columns, 'importance': importances})\n",
    "  feature_importances = feature_importances.sort_values('importance', ascending = False)\n",
    "\n",
    "  # feature_importance에서 150~300개를 선택하여 가장 높은 f1_score를 기록한 개수 및 결과값을 저장\n",
    "  for j in bound : \n",
    "    new_cols = feature_importances[:j]['feature']\n",
    "  \n",
    "    print(\"현재 {} 의 feature_importance로 선택된 컬럼 수는 : {}\".format(model, len(new_cols)))\n",
    "\n",
    "    final_X = new_X[new_cols]\n",
    "\n",
    "    y_pred, f1Score = stacking_and_pred(final_X)\n",
    "  \n",
    "    file_name = f\"submit_code_f1_score={f1Score: .4f},bound={j}.csv\"\n",
    "    submit[\"Y_Class\"] = y_pred\n",
    "    submit.to_csv(\"Untitled Folder/\" +file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa73869-8395-4a12-85a9-780edd87d84b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 RandomForestClassifier(random_state=37) 의 feature_importance로 선택된 컬럼 수는 : 208\n"
     ]
    }
   ],
   "source": [
    "bound = range(208,209)\n",
    "\n",
    "find(X_train, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958d813-6b14-470d-8585-b5f561f76e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
